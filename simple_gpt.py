# -*- coding: utf-8 -*-
"""simple-gpt.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bfwBBl_pr9MnNAfAh5Rlo6F0PRIgzOBb
"""

"""Start by reading some sample text"""

filenames = ["J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt", "J. K. Rowling - Harry Potter 2 - The Chamber Of Secrets.txt", "J. K. Rowling - Harry Potter 3 - Prisoner of Azkaban.txt", "J. K. Rowling - Harry Potter 4 - The Goblet of Fire.txt", "Harry Potter and the Order of the Phoenix.txt", "Bible_KJV.txt", "Harry Potter and The Half-Blood Prince.txt", "Harry Potter and the Deathly Hallows .txt"]
text = ''
for file in filenames:
  with open(file, 'r', encoding='utf-8', errors='replace') as f:
    text += f.read()
print(len(text))
print(text[:1000])

"""Define our list of possible characters - the "vocabulary" of our GPT. The GPT will be able to generate from this list of characters."""

chars = sorted(list(set(text)))
vocab_size = len(chars)
print(''.join(chars))

"""Map our characters to integers for our GPT to parse, and we define some functions to encode our characters to a string, and decode them from an integer."""

stoi = {ch:i for i , ch in enumerate(chars)} # string to integer map
itos = {i:ch for i, ch in enumerate(chars)} # integer to string map
encode = lambda s: [stoi[c] for c in s]
decode = lambda l: ''.join([itos[i] for i in l])
print('Encoded string:', encode('hello world'))
print('Decoded string:', decode(encode('hello world')))

"""We now encode our data using the previously defined map."""

import torch
device = 'cuda' if torch.cuda.is_available() else 'cpu'
data = torch.tensor(encode(text), dtype=torch.long, device=device)
print(data.shape, data.dtype)
print(data[:100])

"""We split our data into a training and testing set - 90% for training and the rest for testing."""

n = int(0.9 * len(data))
train_data = data[:n]
test_data = data[n:]

"""The block size represents the context length - how many characters the model will consider before predicting the next character. Eg. setting the block size to 8 means the model will consider up to 8 characters prior to the last one to predict the next character."""

block_size = 256
train_data[:block_size + 1]

"""We obtain the training data for the model. Based on our block size (context length), we obtain batches of training data. Each batch is randomly generated from some start position in our text. The batch will consist of the input characters to the model, and the correct target outputs that we would like our model to predict. That is, the next character in the sequence"""

batch_size = 64

def get_batch(split):
  data = train_data if split == 'train' else test_data

  batches_start_pos = torch.randint(len(data) - block_size, (batch_size,)) # four randints for start pos

  inputs = torch.stack([data[i:i + block_size] for i in batches_start_pos]) # context chars
  targets = torch.stack([data[i + 1:i + block_size + 1] for i in batches_start_pos]) # target chars

  inputs, targets = inputs.to(device), targets.to(device)

  return inputs, targets

input_batch, target_batch = get_batch('train')
# print('inputs:')
# print(input_batch.shape)
# print(input_batch)
# print('targets:')
# print(target_batch.shape)
# print(target_batch)

for b in range(batch_size):
  for t in range(block_size):
    context = input_batch[b, :t+1]
    target = target_batch[b,t]

import torch.nn as nn
from torch.nn import functional as F

# hyperparams
n_embd = 384 # dimension of each vector for each token - how many data points we want to assign to each token
n_head = 6
n_layer = 6
dropout = 0.2 # percent of nodes dropped by dropout layer

class AttentionHead(nn.Module):
  """Defines a single head of self-attention."""

  def __init__(self, head_size):
    super().__init__()
    self.key = nn.Linear(n_embd, head_size, bias=False)
    self.query = nn.Linear(n_embd, head_size, bias=False)
    self.value = nn.Linear(n_embd, head_size, bias=False)

    # defines a triangular matrix to mask out values that we don't want in training - see readme for details
    self.register_buffer('tril_mask', torch.tril(torch.ones(block_size, block_size)))

    self.dropout = nn.Dropout(dropout)

  def forward(self, x):
    """Accepts an input tensor of dim batch_size * block_size * channels
    Outputs to a tensor of dim batch_size * block_size * head_size"""
    batch, time, channel = x.shape

    # generating key and query vectors for each token
    keys = self.key(x) # dim = (batch, time, head_size)
    queries = self.query(x) # dim = (batch, time, head_size)
    # associate keys and queries to find values with high correlation via matrix mul
    # an additional sqrt term is introduced to decrease variance for softmax - see readme for details
    weights = queries @ keys.transpose(-2, -1) * keys.shape[-1] ** 0.5
    # masks weight values using the previously defined matrix, replaces 0 values with -inf in prep for softmax
    weights = weights.masked_fill(self.tril_mask[:time, :time] == 0, float('-inf'))
    # softmax to convert to normalized probabilities
    weights = F.softmax(weights, dim=-1)

    weights = self.dropout(weights)

    values = self.value(x)

    return weights @ values

class MultiheadAttention(nn.Module):

  def __init__(self, num_heads, head_size):
    super().__init__()
    self.heads = nn.ModuleList([AttentionHead(head_size) for _ in range(num_heads)])
    self.proj = nn.Linear(head_size * num_heads, n_embd) # projects back to number of embeddings
    self.dropout = nn.Dropout(dropout)

  def forward(self, x):
    out = torch.cat([head(x) for head in self.heads], dim=-1)
    return self.dropout(self.proj(out))

class FeedForward(nn.Module):
  """Multilayer perceptron in between attention blocks."""

  def __init__(self, n_embd):
    super().__init__()
    self.net = nn.Sequential(
        nn.Linear(n_embd, 4 * n_embd), # based on paper
        nn.ReLU(),
        nn.Linear(4 * n_embd, n_embd),
        nn.Dropout(dropout)
    )

  def forward(self, x):
    return self.net(x)

class Block(nn.Module):

  def __init__(self, n_embd, num_heads):
    super().__init__()
    head_size = n_embd // num_heads
    self.sa = MultiheadAttention(num_heads, head_size) # self attention layer
    self.ffwd = FeedForward(n_embd) # feed forwad nn layer
    self.ln1 = nn.LayerNorm(n_embd)
    self.ln2 = nn.LayerNorm(n_embd)

  def forward(self, x):
    x = x + self.sa(self.ln1(x))
    x = x + self.ffwd(self.ln2(x))
    return x

class GPT(nn.Module):

  def __init__(self):
    super().__init__()
    self.token_embedding_table = nn.Embedding(vocab_size, n_embd, device=device) # creates a vocab_size * n_embd matrix containing vector representation of each token
    self.position_embedding_table = nn.Embedding(block_size, n_embd) # creates a block_size * n_embd matrix embedding each position in the block
    self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])
    self.ln_f = nn.LayerNorm(n_embd)
    self.final = nn.Linear(n_embd, vocab_size)

    self.apply(self._init_weights)

  def _init_weights(self, module):
    if isinstance(module, nn.Linear):
      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
      if module.bias is not None:
          torch.nn.init.zeros_(module.bias)
    elif isinstance(module, nn.Embedding):
        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)

  def forward(self, idx, targets=None):
    batch, time = idx.shape
    # idx and targets are batch_size * block_size tensors of ints
    # passing these through the embedding table gives the vector equivalents with n_embd number of dimensions
    tok_emb = self.token_embedding_table(idx) # unnormalized logits (vector values) for each token
    pos_emb = self.position_embedding_table(torch.arange(time, device=device))
    x = tok_emb + pos_emb
    x = self.blocks(x)
    x = self.ln_f(x)
    logits = self.final(x)

    if targets is None:
      loss = None
    else:
      batch, time, channel = logits.shape # tensor of ints: batch, time (position), channel (token)
      logits = logits.view(batch * time, channel)
      targets = targets.view(batch * time)
      loss = F.cross_entropy(logits, targets)

    return logits, loss

  def generate(self, idx, max_new_tokens):
    for _ in range(max_new_tokens):
      idx_cond = idx[:, -block_size:]

      logits, loss = self(idx_cond) # loss is not needed here

      logits = logits[:, -1, :] # last element in time dim

      probs = F.softmax(logits, dim=-1) # normalize probs

      idx_next = torch.multinomial(probs, num_samples=1) # sample from dist

      idx = torch.cat((idx, idx_next), dim=1) # append to result

    return idx

model = GPT()
model = model.to(device)
logits, loss = model(input_batch, target_batch)
print(logits.shape)
print(loss)

idx = torch.zeros((1, 1), dtype=torch.long, device=device) # single input char
print(decode(model.generate(idx, max_new_tokens=100)[0].tolist()))

optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)
eval_iters = 200

@torch.no_grad()
def estimate_loss():
    out = {}
    model.eval()
    for split in ['train', 'val']:
        losses = torch.zeros(eval_iters)
        for k in range(eval_iters):
            X, Y = get_batch(split)
            logits, loss = model(X, Y)
            losses[k] = loss.item()
        out[split] = losses.mean()
    model.train()
    return out

max_iters = 500

for iter in range(max_iters):
  if iter % 50 == 0 or iter == max_iters - 1:
    losses = estimate_loss()
    print(f"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}")

  inputs, targets = get_batch('train')

  logits, loss = model(inputs, targets)
  optimizer.zero_grad(set_to_none=True)
  loss.backward()
  optimizer.step()

losses = estimate_loss()
print(f"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}")

print(decode(model.generate(torch.zeros((1, 1), dtype=torch.long, device=device), max_new_tokens=300)[0].tolist()))