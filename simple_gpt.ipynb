{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fh8YicchK7RE",
        "outputId": "6233016d-e900-4a0e-994f-5eecf418f0ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-14 03:35:15--  https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%201%20-%20Sorcerer's%20Stone.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 439742 (429K) [text/plain]\n",
            "Saving to: ‘J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt’\n",
            "\n",
            "\r          J. K. Row   0%[                    ]       0  --.-KB/s               \rJ. K. Rowling - Har 100%[===================>] 429.44K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-06-14 03:35:15 (9.86 MB/s) - ‘J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt’ saved [439742/439742]\n",
            "\n",
            "--2024-06-14 03:35:15--  https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%202%20-%20The%20Chamber%20Of%20Secrets.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 492130 (481K) [text/plain]\n",
            "Saving to: ‘J. K. Rowling - Harry Potter 2 - The Chamber Of Secrets.txt’\n",
            "\n",
            "J. K. Rowling - Har 100%[===================>] 480.60K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-06-14 03:35:16 (8.63 MB/s) - ‘J. K. Rowling - Harry Potter 2 - The Chamber Of Secrets.txt’ saved [492130/492130]\n",
            "\n",
            "--2024-06-14 03:35:16--  https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%203%20-%20Prisoner%20of%20Azkaban.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 611601 (597K) [text/plain]\n",
            "Saving to: ‘J. K. Rowling - Harry Potter 3 - Prisoner of Azkaban.txt’\n",
            "\n",
            "J. K. Rowling - Har 100%[===================>] 597.27K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-06-14 03:35:16 (12.5 MB/s) - ‘J. K. Rowling - Harry Potter 3 - Prisoner of Azkaban.txt’ saved [611601/611601]\n",
            "\n",
            "--2024-06-14 03:35:16--  https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%204%20-%20The%20Goblet%20of%20Fire.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1109179 (1.1M) [application/octet-stream]\n",
            "Saving to: ‘J. K. Rowling - Harry Potter 4 - The Goblet of Fire.txt’\n",
            "\n",
            "J. K. Rowling - Har 100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-06-14 03:35:17 (17.7 MB/s) - ‘J. K. Rowling - Harry Potter 4 - The Goblet of Fire.txt’ saved [1109179/1109179]\n",
            "\n",
            "--2024-06-14 03:35:17--  https://raw.githubusercontent.com/amephraim/nlp/master/texts/Bible_KJV.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4452069 (4.2M) [text/plain]\n",
            "Saving to: ‘Bible_KJV.txt’\n",
            "\n",
            "Bible_KJV.txt       100%[===================>]   4.25M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2024-06-14 03:35:17 (49.2 MB/s) - ‘Bible_KJV.txt’ saved [4452069/4452069]\n",
            "\n",
            "--2024-06-14 03:35:17--  https://raw.githubusercontent.com/bobdeng/owlreader/master/ERead/assets/books/Harry%20Potter%20and%20the%20Order%20of%20the%20Phoenix.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1524926 (1.5M) [text/plain]\n",
            "Saving to: ‘Harry Potter and the Order of the Phoenix.txt’\n",
            "\n",
            "Harry Potter and th 100%[===================>]   1.45M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-06-14 03:35:18 (22.2 MB/s) - ‘Harry Potter and the Order of the Phoenix.txt’ saved [1524926/1524926]\n",
            "\n",
            "--2024-06-14 03:35:18--  https://raw.githubusercontent.com/bobdeng/owlreader/master/ERead/assets/books/Harry%20Potter%20and%20The%20Half-Blood%20Prince.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 993708 (970K) [text/plain]\n",
            "Saving to: ‘Harry Potter and The Half-Blood Prince.txt’\n",
            "\n",
            "Harry Potter and Th 100%[===================>] 970.42K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-06-14 03:35:18 (17.2 MB/s) - ‘Harry Potter and The Half-Blood Prince.txt’ saved [993708/993708]\n",
            "\n",
            "--2024-06-14 03:35:18--  https://raw.githubusercontent.com/bobdeng/owlreader/master/ERead/assets/books/Harry%20Potter%20and%20the%20Deathly%20Hallows%20.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1187271 (1.1M) [text/plain]\n",
            "Saving to: ‘Harry Potter and the Deathly Hallows .txt’\n",
            "\n",
            "Harry Potter and th 100%[===================>]   1.13M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-06-14 03:35:18 (17.5 MB/s) - ‘Harry Potter and the Deathly Hallows .txt’ saved [1187271/1187271]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget \"https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%201%20-%20Sorcerer's%20Stone.txt\"\n",
        "!wget https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%202%20-%20The%20Chamber%20Of%20Secrets.txt\n",
        "!wget https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%203%20-%20Prisoner%20of%20Azkaban.txt\n",
        "!wget https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%204%20-%20The%20Goblet%20of%20Fire.txt\n",
        "!wget https://raw.githubusercontent.com/amephraim/nlp/master/texts/Bible_KJV.txt\n",
        "!wget https://raw.githubusercontent.com/bobdeng/owlreader/master/ERead/assets/books/Harry%20Potter%20and%20the%20Order%20of%20the%20Phoenix.txt\n",
        "!wget https://raw.githubusercontent.com/bobdeng/owlreader/master/ERead/assets/books/Harry%20Potter%20and%20The%20Half-Blood%20Prince.txt\n",
        "!wget https://raw.githubusercontent.com/bobdeng/owlreader/master/ERead/assets/books/Harry%20Potter%20and%20the%20Deathly%20Hallows%20.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start by reading some sample text"
      ],
      "metadata": {
        "id": "tlqRRzp4hmMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filenames = [\"J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt\", \"J. K. Rowling - Harry Potter 2 - The Chamber Of Secrets.txt\", \"J. K. Rowling - Harry Potter 3 - Prisoner of Azkaban.txt\", \"J. K. Rowling - Harry Potter 4 - The Goblet of Fire.txt\", \"Harry Potter and the Order of the Phoenix.txt\", \"Bible_KJV.txt\", \"Harry Potter and The Half-Blood Prince.txt\", \"Harry Potter and the Deathly Hallows .txt\"]\n",
        "text = ''\n",
        "for file in filenames:\n",
        "  with open(file, 'r', encoding='utf-8', errors='replace') as f:\n",
        "    text += f.read()\n",
        "print(len(text))\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFvdBkltLRbP",
        "outputId": "1b3eea4e-8f3e-4bca-91bc-440715e5a333"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10683178\n",
            "Harry Potter and the Sorcerer's Stone\n",
            "\n",
            "\n",
            "CHAPTER ONE\n",
            "\n",
            "THE BOY WHO LIVED\n",
            "\n",
            "Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say\n",
            "that they were perfectly normal, thank you very much. They were the last\n",
            "people you'd expect to be involved in anything strange or mysterious,\n",
            "because they just didn't hold with such nonsense.\n",
            "\n",
            "Mr. Dursley was the director of a firm called Grunnings, which made\n",
            "drills. He was a big, beefy man with hardly any neck, although he did\n",
            "have a very large mustache. Mrs. Dursley was thin and blonde and had\n",
            "nearly twice the usual amount of neck, which came in very useful as she\n",
            "spent so much of her time craning over garden fences, spying on the\n",
            "neighbors. The Dursleys had a small son called Dudley and in their\n",
            "opinion there was no finer boy anywhere.\n",
            "\n",
            "The Dursleys had everything they wanted, but they also had a secret, and\n",
            "their greatest fear was that somebody would discover it. They didn't\n",
            "think they could bear it if anyone found out about the Potters. Mr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define our list of possible characters - the \"vocabulary\" of our GPT. The GPT will be able to generate from this list of characters."
      ],
      "metadata": {
        "id": "iJC_2eGzhr7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpQqPDFmLev1",
        "outputId": "97f3876f-d129-4fd5-f7f5-bd4749dbedbb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t\n",
            "\f\u001f !\"#$%&'()*,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz|}~é–—‘’“”…﻿�\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Map our characters to integers for our GPT to parse, and we define some functions to encode our characters to a string, and decode them from an integer."
      ],
      "metadata": {
        "id": "ewxw3dNfh3wh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = {ch:i for i , ch in enumerate(chars)} # string to integer map\n",
        "itos = {i:ch for i, ch in enumerate(chars)} # integer to string map\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "print('Encoded string:', encode('hello world'))\n",
        "print('Decoded string:', decode(encode('hello world')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlRlMxcELr2G",
        "outputId": "e18fd902-8bbf-4dda-ad57-7c6b01558a28"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded string: [75, 72, 79, 79, 82, 4, 90, 82, 85, 79, 71]\n",
            "Decoded string: hello world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now encode our data using the previously defined map."
      ],
      "metadata": {
        "id": "xx52JBU2ilVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "data = torch.tensor(encode(text), dtype=torch.long, device=device)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rhcx745JMU4F",
        "outputId": "bea45e96-5340-4f16-879a-d481f95df4d1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10683178]) torch.int64\n",
            "tensor([43, 68, 85, 85, 92,  4, 51, 82, 87, 87, 72, 85,  4, 68, 81, 71,  4, 87,\n",
            "        75, 72,  4, 54, 82, 85, 70, 72, 85, 72, 85, 11, 86,  4, 54, 87, 82, 81,\n",
            "        72,  1,  1,  1, 38, 43, 36, 51, 55, 40, 53,  4, 50, 49, 40,  1,  1, 55,\n",
            "        43, 40,  4, 37, 50, 60,  4, 58, 43, 50,  4, 47, 44, 57, 40, 39,  1,  1,\n",
            "        48, 85, 17,  4, 68, 81, 71,  4, 48, 85, 86, 17,  4, 39, 88, 85, 86, 79,\n",
            "        72, 92, 15,  4, 82, 73,  4, 81, 88, 80])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We split our data into a training and testing set - 90% for training and the rest for testing."
      ],
      "metadata": {
        "id": "yoSTl5cyix3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "test_data = data[n:]"
      ],
      "metadata": {
        "id": "bYif3lv8NLA5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The block size represents the context length - how many characters the model will consider before predicting the next character. Eg. setting the block size to 8 means the model will consider up to 8 characters prior to the last one to predict the next character."
      ],
      "metadata": {
        "id": "RE6CIb-8jLS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 256\n",
        "train_data[:block_size + 1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFv2w6eYNQus",
        "outputId": "21cd3064-792b-48a7-bad4-4370e285cafa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([43, 68, 85, 85, 92,  4, 51, 82, 87, 87, 72, 85,  4, 68, 81, 71,  4, 87,\n",
              "        75, 72,  4, 54, 82, 85, 70, 72, 85, 72, 85, 11, 86,  4, 54, 87, 82, 81,\n",
              "        72,  1,  1,  1, 38, 43, 36, 51, 55, 40, 53,  4, 50, 49, 40,  1,  1, 55,\n",
              "        43, 40,  4, 37, 50, 60,  4, 58, 43, 50,  4, 47, 44, 57, 40, 39,  1,  1,\n",
              "        48, 85, 17,  4, 68, 81, 71,  4, 48, 85, 86, 17,  4, 39, 88, 85, 86, 79,\n",
              "        72, 92, 15,  4, 82, 73,  4, 81, 88, 80, 69, 72, 85,  4, 73, 82, 88, 85,\n",
              "        15,  4, 51, 85, 76, 89, 72, 87,  4, 39, 85, 76, 89, 72, 15,  4, 90, 72,\n",
              "        85, 72,  4, 83, 85, 82, 88, 71,  4, 87, 82,  4, 86, 68, 92,  1, 87, 75,\n",
              "        68, 87,  4, 87, 75, 72, 92,  4, 90, 72, 85, 72,  4, 83, 72, 85, 73, 72,\n",
              "        70, 87, 79, 92,  4, 81, 82, 85, 80, 68, 79, 15,  4, 87, 75, 68, 81, 78,\n",
              "         4, 92, 82, 88,  4, 89, 72, 85, 92,  4, 80, 88, 70, 75, 17,  4, 55, 75,\n",
              "        72, 92,  4, 90, 72, 85, 72,  4, 87, 75, 72,  4, 79, 68, 86, 87,  1, 83,\n",
              "        72, 82, 83, 79, 72,  4, 92, 82, 88, 11, 71,  4, 72, 91, 83, 72, 70, 87,\n",
              "         4, 87, 82,  4, 69, 72,  4, 76, 81, 89, 82, 79, 89, 72, 71,  4, 76, 81,\n",
              "         4, 68, 81, 92, 87])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We obtain the training data for the model. Based on our block size (context length), we obtain batches of training data. Each batch is randomly generated from some start position in our text. The batch will consist of the input characters to the model, and the correct target outputs that we would like our model to predict. That is, the next character in the sequence"
      ],
      "metadata": {
        "id": "CTAHfa7OitTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else test_data\n",
        "\n",
        "  batches_start_pos = torch.randint(len(data) - block_size, (batch_size,)) # four randints for start pos\n",
        "\n",
        "  inputs = torch.stack([data[i:i + block_size] for i in batches_start_pos]) # context chars\n",
        "  targets = torch.stack([data[i + 1:i + block_size + 1] for i in batches_start_pos]) # target chars\n",
        "\n",
        "  inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "  return inputs, targets\n",
        "\n",
        "input_batch, target_batch = get_batch('train')\n",
        "# print('inputs:')\n",
        "# print(input_batch.shape)\n",
        "# print(input_batch)\n",
        "# print('targets:')\n",
        "# print(target_batch.shape)\n",
        "# print(target_batch)\n",
        "\n",
        "for b in range(batch_size):\n",
        "  for t in range(block_size):\n",
        "    context = input_batch[b, :t+1]\n",
        "    target = target_batch[b,t]\n"
      ],
      "metadata": {
        "id": "-u5r0gZmOV59"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "h28aO37LliTC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparams\n",
        "n_embd = 384 # dimension of each vector for each token - how many data points we want to assign to each token\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2 # percent of nodes dropped by dropout layer"
      ],
      "metadata": {
        "id": "GN_f3QU7lwpe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "  \"\"\"Defines a single head of self-attention.\"\"\"\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "\n",
        "    # defines a triangular matrix to mask out values that we don't want in training - see readme for details\n",
        "    self.register_buffer('tril_mask', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"Accepts an input tensor of dim batch_size * block_size * channels\n",
        "    Outputs to a tensor of dim batch_size * block_size * head_size\"\"\"\n",
        "    batch, time, channel = x.shape\n",
        "\n",
        "    # generating key and query vectors for each token\n",
        "    keys = self.key(x) # dim = (batch, time, head_size)\n",
        "    queries = self.query(x) # dim = (batch, time, head_size)\n",
        "    # associate keys and queries to find values with high correlation via matrix mul\n",
        "    # an additional sqrt term is introduced to decrease variance for softmax - see readme for details\n",
        "    weights = queries @ keys.transpose(-2, -1) * keys.shape[-1] ** 0.5\n",
        "    # masks weight values using the previously defined matrix, replaces 0 values with -inf in prep for softmax\n",
        "    weights = weights.masked_fill(self.tril_mask[:time, :time] == 0, float('-inf'))\n",
        "    # softmax to convert to normalized probabilities\n",
        "    weights = F.softmax(weights, dim=-1)\n",
        "\n",
        "    weights = self.dropout(weights)\n",
        "\n",
        "    values = self.value(x)\n",
        "\n",
        "    return weights @ values\n"
      ],
      "metadata": {
        "id": "Y4Iim_XYrdxk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([AttentionHead(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(head_size * num_heads, n_embd) # projects back to number of embeddings\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "    return self.dropout(self.proj(out))\n",
        "\n"
      ],
      "metadata": {
        "id": "70iiJaesX6a2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  \"\"\"Multilayer perceptron in between attention blocks.\"\"\"\n",
        "\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, 4 * n_embd), # based on paper\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * n_embd, n_embd),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "cqm9aTkCYh2g"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "  def __init__(self, n_embd, num_heads):\n",
        "    super().__init__()\n",
        "    head_size = n_embd // num_heads\n",
        "    self.sa = MultiheadAttention(num_heads, head_size) # self attention layer\n",
        "    self.ffwd = FeedForward(n_embd) # feed forwad nn layer\n",
        "    self.ln1 = nn.LayerNorm(n_embd)\n",
        "    self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "MglP7J0_ZQkl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd, device=device) # creates a vocab_size * n_embd matrix containing vector representation of each token\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd) # creates a block_size * n_embd matrix embedding each position in the block\n",
        "    self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(n_embd)\n",
        "    self.final = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "      if module.bias is not None:\n",
        "          torch.nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    batch, time = idx.shape\n",
        "    # idx and targets are batch_size * block_size tensors of ints\n",
        "    # passing these through the embedding table gives the vector equivalents with n_embd number of dimensions\n",
        "    tok_emb = self.token_embedding_table(idx) # unnormalized logits (vector values) for each token\n",
        "    pos_emb = self.position_embedding_table(torch.arange(time, device=device))\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.blocks(x)\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.final(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      batch, time, channel = logits.shape # tensor of ints: batch, time (position), channel (token)\n",
        "      logits = logits.view(batch * time, channel)\n",
        "      targets = targets.view(batch * time)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "\n",
        "      logits, loss = self(idx_cond) # loss is not needed here\n",
        "\n",
        "      logits = logits[:, -1, :] # last element in time dim\n",
        "\n",
        "      probs = F.softmax(logits, dim=-1) # normalize probs\n",
        "\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # sample from dist\n",
        "\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # append to result\n",
        "\n",
        "    return idx\n",
        "\n"
      ],
      "metadata": {
        "id": "etHrEe62gD1d"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT()\n",
        "model = model.to(device)\n",
        "logits, loss = model(input_batch, target_batch)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "idx = torch.zeros((1, 1), dtype=torch.long, device=device) # single input char\n",
        "print(decode(model.generate(idx, max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wg4CXzUftKkQ",
        "outputId": "71e53ddc-993c-4a13-bdc4-9d89f6051241"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16384, 107])\n",
            "tensor(4.7515, grad_fn=<NllLossBackward0>)\n",
            "\t gOPF*w�Esé\f,\\–}]zu\u001fZ\tn@R<…W“;R^/�)–K.(yy wI…'…\fgo…k7q_\f4OP6(xd–g3h Z﻿\".tL!“L=&7$8GJ“;8d ^T=j>)U1k“%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "eval_iters = 200\n"
      ],
      "metadata": {
        "id": "Lvf4C777r-vs"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "9gvwz3NSfyAV"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_iters = 500"
      ],
      "metadata": {
        "id": "HKJ--TC427dV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(max_iters):\n",
        "  if iter % 50 == 0 or iter == max_iters - 1:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  inputs, targets = get_batch('train')\n",
        "\n",
        "  logits, loss = model(inputs, targets)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LvKqtLasjrx",
        "outputId": "2edbeeaf-20f8-4d5f-b057-62f93beaa1f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 2.5723, val loss 2.6663\n",
            "step 50: train loss 2.5163, val loss 2.6342\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "losses = estimate_loss()\n",
        "print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")"
      ],
      "metadata": {
        "id": "IcNpUENN0QY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(model.generate(torch.zeros((1, 1), dtype=torch.long, device=device), max_new_tokens=300)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlOE31ertL56",
        "outputId": "187c425b-3670-4ab7-eecb-3ebd7a00f921"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tbnctud fond  as d..-3 acig y sonbt Dof f f Pr h.?Epave, s Aced: ayssimid warg hig bed tre wa, f tame tokerlud tch as8 Ihagry thad andimaand thtoinds sethathe hapiserighan thillayey ghere shecat.:rd ad Unig s Iy me ct mwanveoreming shee rmaafrine tote tof halis, Hed ape jund  l as nghLastusan ome h, \n"
          ]
        }
      ]
    }
  ]
}